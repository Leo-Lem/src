\documentclass{article}

\usepackage{summary}

\subject{Algorithms and Data Structures}
\semester{Winter 2023/24}
\author{Leopold Lemmermann}

\begin{document}\createtitle

\section{The Asymptotic View}
\subsection{Bachmann-Landau Notation}
\begin{itemize}
  \item[$\bigO$] (big O: upper bound) $f\in \bigO(g) \Leftrightarrow ^{\exists c > 0}_{\exists n_0 > 0}: f \leq c \cdot g \forall n > n_0$ \textbf{or} $\limsup_{n\to\infty} \frac{f}{g} < \infty$
  \item[$\Omega$] (big Omega: lower bound) $f\in \Omega(g) \Leftrightarrow ^{\exists c > 0}_{\exists n_0 > 0}: f \geq c \cdot g \forall n > n_0$ \textbf{or} $\liminf_{n\to\infty} \frac{f}{g} > 0$
  \item[$\littleO$] (little o: strict upper bound) $f\in \littleO(g) \Leftrightarrow ^{\forall c > 0}_{\exists n_0 > 0}: f < c \cdot g \forall n > n_0$ \textbf{or} $\lim_{n\to\infty} \frac{f}{g} = 0$
  \item[$\omega$] (little omega: strict lower bound) $f\in \omega(g) \Leftrightarrow ^{\forall c > 0}_{\exists n_0 > 0}: f > c \cdot g \forall n > n_0$ \textbf{or} $\lim_{n\to\infty} \frac{f}{g} = \infty$
  \item[$\Theta$] (big Theta: tight bound) $f\in \Theta(g) \Leftrightarrow f\in \bigO(g) \land f\in \Omega(g)$
\end{itemize}

\subsection{Master Theorem}
The master theorem is a method for solving recurrences of the form
\input{res/master.eqt}
where $a \geq 1$, $b > 1$, $f(n)$ is a given function, and $n_0$ is a given constant.

\begin{itemize}
  \item[I.] (recursion dominates) $\exists \epsilon > 0: f(n) \in \bigO(n^{\log_b(a)-\epsilon}) \Rightarrow T(n) \in \Theta(n^{\log_b a})$
  \item[II.] (recursion and non-recursion are balanced) $\exists k \ge 0: f(n) \in \Theta(n^{\log_b a}\log^k n) \Rightarrow T(n) \in \Theta(n^{\log_b a} \log^{k+1} n)$
  \item[III.] (non-recursion dominates) $\underbrace{^{\exists \epsilon > 0: f(n) \in \Omega(n^{\log_b(a)+\epsilon})}_{\exists c \in [0\dots1]: a f(\frac{n}{b}) \leq c f(n)}}_{\text{\tiny regularity condition}} \Rightarrow T(n) \in \Theta(f(n))$
\end{itemize}



\section{Sorting}
\subsection{Elementary sorting algorithms}
The elementary sorting algorithms \algref{selectionSort} \algref{insertionSort} and \algref{bubbleSort} have a time complexity of $\bigO(n^2)$. They sort stable and in-place.
\subsubsection{Selection Sort}
Continue selecting the smallest element and swapping it with the first unsorted element.\par
\input{res/sort/selectionsort.alg}

\subsubsection{Insertion Sort}
Continue inserting the first unsorted element into the sorted part of the array.\par
\input{res/sort/insertionsort.alg}

\subsubsection{Bubble Sort}
Continue swapping adjacent elements if they are in the wrong order.\par
\input{res/sort/bubblesort.alg}

\subsection{Divide \& Conquer}
The divide and conquer sorting algorithms \algref{mergeSort} and \algref{quickSort} have a time complexity of $\bigO(n\log n)$.
\subsubsection{Merge Sort}
\textbf{Idea}: Divide the array into two halves, sort the halves, and then merge them.\par
\textbf{Properties}: Sorts stable and not in-place.\par
\input{res/sort/mergesort.alg}

\subsubsection{Quick Sort}
\textbf{Idea}: Choose a pivot element and partition the array into elements smaller and larger than the pivot. Then sort the partitions.\par
\textbf{Properties}: Sorts in-place, but not stable.\par
\input{res/sort/quicksort.alg}

\subsection{Heap Sort}
\textbf{Idea}: Use heap structure to sort the array.\par
\textbf{Properties}: Sorts in-place, but not stable.\par
\textbf{Complexity}: The time complexity is $\bigO(n\log n)$.\par
\input{res/sort/heapsort.alg}

\subsection{Non-comparing Sorting}
\subsubsection{Trivial Sort}
\textbf{Assumption}: Small range of input, pairwise different, integers.\par
\textbf{Idea}: Use a lookup table to sort the input.\par
\textbf{Complexity}: Timewise $\bigO(n)$, but requires $\bigO(k)$ space, where $k$ is the range of the input.\par
\input{res/sort/trivialsort.alg}

\subsubsection{Counting Sort}
\textbf{Assumption}: Integers up to $k\in\bigO(n)$.\par
\textbf{Idea}: Count the number of occurrences of each element and then reconstruct the sorted array.\par
\textbf{Complexity}: Timewise $\bigO(n+k)$, where $k$ is the range of the input.\par
\input{res/sort/countingsort.alg}

\subsubsection{Radix Sort}
\textbf{Assumption}: Integers of base $m$ with $d$ digits lesser than $m-1$.\par
\textbf{Idea}: Sort the elements by their digits from the least significant to the most significant.\par
\textbf{Complexity}: Timewise $\bigO(d(n+m))$, where $d$ is the number of digits and $m$ is the base of the integers.\par
\input{res/sort/radixsort.alg}

\subsubsection{Bucket Sort}
\textbf{Assumption}: Uniformly distributed numbers in $[0,1)$.\par
\textbf{Idea}: Distribute the elements into $n$ buckets and then sort each bucket.\par
\textbf{Complexity}: Timewise $\bigO(n^2)$, but $\bigO(n)$ with a good choice of $n$.\par
\input{res/sort/bucketsort.alg}



\section{Searching}
\subsection{Algorithms}
\subsubsection{Linear Search}
\textbf{Idea}: Iterate through the array and compare each element with the search key.\par
\textbf{Complexity}: Timewise $\bigO(n)$.\par
\input{res/search/linearsearch.alg}

\subsubsection{Binary Search}
\textbf{Idea}: Divide the array into two halves and continue searching in the half that contains the search key.\par
\textbf{Complexity}: Timewise $\bigO(\log n)$.\par
\input{res/search/binarysearch.alg}

\subsubsection{Exponential Search}
\textbf{Idea}: Double the search range until the search key is found or the range is larger than the array.\par
\textbf{Complexity}: Timewise $\bigO(\log n)$.\par
\input{res/search/exponentialsearch.alg}

\subsection{Selection}
\subsubsection{Quick Select}
\textbf{Idea}: Based on \algref{quickSort}, but only recursively sort the partition that contains the $i$-th order statistic.\par
\textbf{Complexity}: Timewise $\bigO(n)$ on average, $\bigO(n^2)$ in the worst case.\par
\input{res/search/quickselect.alg}

\subsubsection{Randomized Selection}
\textbf{Idea}: \algrefp{quickSelect} with randomized partition.\par
\textbf{Complexity}: Timewise $\bigO(n)$ on average, $\bigO(n^2)$ in the worst case.\par

\subsubsection{Median of Medians}
\textbf{Idea}: Divide the array into groups of 5 elements, find the median of each group, and then recursively find the median of the medians.\par
\textbf{Complexity}: Timewise $\bigO(n)$.\par

\subsection{Red-Black Trees}
\subsubsection{Properties}
\begin{itemize}
  \item[I.]\label{properties:1} [Colored] Every node is either red or black.
  \item[II.]\label{properties:2} [Black Root] The root is black.
  \item[III.]\label{properties:3} [Black Leaves] Every leaf (NIL) is black.
  \item[IV.]\label{properties:4} [Red $\to$ Black Children] If a red node has children, then the children are black.
  \item[V.]\label{properties:5} [Black Height] For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.
\end{itemize}

\subsubsection{Insertion}
\input{res/rb/insert.alg}

\subsubsection{Deletion}
\input{res/rb/delete.alg}

\subsection{AVL Trees}
\subsubsection{Property}
For every node, the heights of the left and right subtrees differ by at most 1.

\subsubsection{Insertion}
Insert the new node as in a normal binary search tree and then rebalance the tree by performing rotations from the inserted node to the root.

\subsubsection{Deletion}
Delete the node as in a normal binary search tree and then rebalance the tree by performing rotations from the deleted node to the root.

\subsection{Hashing}
\subsubsection{Hash Functions}
\begin{enumerate}
  \item Division Method: $h(k) = k \mod m$
  \item Multiplication Method: $h(k) = \lfloor m \cdot (kA \mod 1) \rfloor$
  \item Universal Hashing (Extended Division method): $h(k) = ((ak+b) \mod p) \mod m$
\end{enumerate}

\subsubsection{Chaining}\label{sec:chaining}
Chained hashing uses a linked list to store all elements that hash to the same value. The time complexity of chained hashing is $\bigO(1+\alpha)$, where $\alpha$ is the load factor.

\subsubsection{Open Addressing}\label{sec:openAddressing}
Open addressing uses the next free slot in the hash table to store elements that hash to the same value. The time complexity of open addressing is $\bigO(1+\alpha)$, where $\alpha$ is the load factor.
\begin{enumerate}
  \item Linear Probing: $h(k,i) = (h'(k) + i) \mod m$
  \item Quadratic Probing: $h(k,i) = (h'(k) + c_1\cdot i + c_2\cdot i^2) \mod m$
  \item Double Hashing: $h(k,i) = (h_1(k) + i\cdot h_2(k)) \mod m$
\end{enumerate}



\section{Graphs}
\subsection{Topological Sort}
\input{res/graphs/topo.alg}

\subsection{Strongly Connected Components: Kosaraju-Sharir's Algorithm}
\input{res/graphs/sccs.alg}

\subsection{Minimal Spanning Trees}
\subsubsection{Kruskal}
The \algrefp{kruskal} uses the 'lightest' edge between trees to find the MST. The algorithm has a time complexity of $\bigO(E\log V)$.\par

\input{res/mst/kruskal.alg.tex}

\subsubsection{Prim}
The \algrefp{prim} algorithm starts with an arbitrary vertex and adds the 'lightest' edge to the MST. The algorithm has a time complexity of $\bigO(E\log V)$ when using an adjacency list.\par
\input{res/mst/prim.alg.tex}

\subsection{Find Bridges}
The \algrefp{findBridges} algorithm finds all bridges in a graph. A bridge is an edge whose removal increases the number of connected components in the graph. The algorithm has a time complexity of $\bigO(E(V+E))=\bigO(E\cdot V+E^2)$.\par
\input{res/graphs/findBridges.alg}
The DFS algorithm \algrefp{findICC} finds an initial connected component (ICC) of a graph $G$ starting from a vertex $u$. The algorithm (like other DFSs) has a time complexity of $\bigO(V+E)$.\par
\input{res/graphs/findICC.alg}

\subsection{Shortest Paths}
\input{res/shortestpaths/basics.alg}

\subsubsection{Bellman-Ford}
The \algrefp{bellmanFord} finds the shortest path from a source vertex $s$ to all other vertices in a graph $G$. The algorithm has a time complexity of $\bigO(V\cdot E)$.\par
\input{res/shortestpaths/bellmanford.alg}

\subsubsection{Dijkstra}
The \algrefp{dijkstra} algorithm finds the shortest path from a source vertex $s$ to all other vertices in a graph $G$. The algorithm has a time complexity of $\bigO(E\log V)$ when the Queue is implemented with a Min-Heap. Using Fibonacci-Heaps the time complexity can be reduced to $\bigO(E+V\log V)$.\par
\input{res/shortestpaths/dijkstra.alg}

The \algref{dijkstraSinglePair} finds the shortest path from a source vertex $s$ to a target vertex $t$ in a graph $G$. The algorithm has a time complexity of $\bigO(V\log V+E)+\bigO(V)=\bigO(V\log V+E)$ (based on the single source \texttt{dijsktra}).\par
\input{res/shortestpaths/dijkstrasp.alg}

\subsubsection{Floyd-Warshall}
The \algref{floydWarshall} algorithm finds the shortest path between all pairs of vertices in a graph $G$. The algorithm has a time complexity of $\bigO(V^3)$.\par
\input{res/shortestpaths/floydwarshall.alg}



\section{Hard problems with Smart Solutions}
\subsection{Dynamic Programming}
Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable to problems exhibiting the property of overlapping subproblems and optimal substructure.

The four steps to solve a problem using dynamic programming are:
\begin{enumerate}
  \item[1.] Characterize the structure of an optimal solution.
  \item[2.] Recursively define the value of an optimal solution.
  \item[3.] Compute the value of an optimal solution in a bottom-up fashion.
  \item[4.] Construct an optimal solution from computed information.
\end{enumerate}

\subsection{NP-Completeness}
A decision problem is NP-complete if it is in NP and every problem in NP can be reduced to it in polynomial time. The most famous NP-complete problem is the Boolean satisfiability problem (SAT).

To prove NP-completeness, one can use the following steps:
\begin{enumerate}
  \item Prove that the problem is in NP.
  \item Choose a known NP-complete problem and reduce it to the problem at hand.
\end{enumerate}

\subsection{Solving NP-Complete Problems}
\begin{enumerate}
  \item[1.] \textbf{Approximation Algorithms:} Solution close to optimal by a ratio $\rho(n) = \frac{\text{approximate solution}}{\text{optimal solution}}$. Greedy algorithms, etc.
  \item[2.] \textbf{Heuristic Algorithms:} No guarantee, but resonably good solutions usually. Local search, genetic algorithms, etc.
  \item[3.] \textbf{Exact Algorithms:} Find the optimal solution. Backtracking, branch and bound, etc.
\end{enumerate}
\subsubsection{Known NP-Complete Problems}
\begin{itemize}
  \item \textproblem{SAT (Boolean Satisfiability Problem)}: Given a Boolean formula, is there an assignment of truth values to the variables that makes the formula true?
  \item \textproblem{3-SAT}: Given a Boolean formula in conjunctive normal form, is there a satisfying assignment in which each clause contains at most 3 literals?
  \item \textproblem{k-Clique}: Given a graph $G = (V,E)$ and an integer $k$, is there a clique of size $k$ in $G$?
  \item \textproblem{Subset Sum}: Given a set of integers, is there a subset that sums to a given integer?
\end{itemize}



\section{Runtime Complexity}
\subsection{Sorting}
\begin{itemize}
  \item \algrefp{selectionSort}: $\bigO(n^2)$
  \item \algrefp{insertionSort}: $\bigO(n^2)$
  \item \algrefp{bubbleSort}: $\bigO(n^2)$
  \item \algrefp{mergeSort}: $\bigO(n\log n)$
  \item \algrefp{quickSort}: $\bigO_\text{\O}(n\log n)$, $\bigO(n^2)$
  \item \algrefp{heapSort}: $\bigO(n\log n)$
  \item \algrefp{countingSort}: $\bigO(n+k)$ ($k$: range of input)
  \item \algrefp{radixSort}: $\bigO(d(n+m))$ ($d$: number of digits, $m$: base of integers)
  \item \algrefp{bucketSort}: $\bigO(n^2)$ ($\bigO(n)$ with good choice of $n$)
\end{itemize}

\subsection{Searching}
\begin{itemize}
  \item \algrefp{linearSearch}: $\bigO(n)$
  \item \algrefp{binarySearch}: $\bigO(\log n)$
  \item \algrefp{exponentialSearch}: $\bigO(\log k)$ ($k$: index of the search key)
\end{itemize}
\subsubsection{Trees}
\begin{itemize}
  \item Binary Search Tree: $\bigO(h)$ ($h$: height of the tree)
  \item Red-Black \& AVL Tree: $\bigO(\log n)$
  \item Red-Black \& AVL Tree: Insert, Delete, Min, Max, …: $\bigO(\log n)$
\end{itemize}
\subsubsection{Hashing}
\begin{itemize}
  \item Chaining: $\bigO(1+\alpha)$ ($\alpha$: load factor)
  \item Open Addressing: $\bigO(1+\alpha)$ ($\alpha$: load factor)
\end{itemize}

\subsection{Selection}
\begin{itemize}
  \item \algrefp{quickSelect}: $\bigO(n\log n)$
  \item \textalgo{randSelect}: $\bigO_\text{\O}(n)$, $\bigO(n^2)$
  \item \textalgo{medianOfMedians}: $\bigO(n)$
\end{itemize}

\subsection{Graphs}
\begin{itemize}
  \item \algrefp{topologicalSort}: $\bigO(V+E)$
  \item \textalgo{kosarajuSharir} (SCCs): $\bigO(V+E)$
  \item \algrefp{findBridges}: $\bigO(E(V+E))=\bigO(E\cdot V+E^2)$
\end{itemize}
\subsubsection{MST}
\begin{itemize}
  \item \algrefp{kruskal}: $\bigO(E\log V)$
  \item \algrefp{prim}: $\bigO(E\log V)$
\end{itemize}
\subsubsection{Shortest Paths}
\begin{itemize}
  \item \algrefp{bellmanFord} (single source): $\bigO(V\cdot E)$
  \item \algrefp{dijkstra} (single source): $\bigO(E\log V)$
  \item \algrefp{dijkstraSinglePair} (single-pair): $\bigO(V\log V+E)$
  \item \algrefp{floydWarshall} (all-pairs): $\bigO(V^3)$
\end{itemize}

\end{document}